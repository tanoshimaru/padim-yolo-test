import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import List

from anomalib.data import datamodules, Folder
from anomalib.engine import Engine
from anomalib.models import Padim


def setup_logging() -> logging.Logger:
    """ãƒ­ã‚°è¨­å®š"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    log_filename = f"train_padim_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_dir / log_filename, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
        force=True,
    )
    return logging.getLogger(__name__)


def count_images_in_directory(directory: Path) -> int:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    if not directory.exists():
        return 0

    image_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"}
    count = 0

    for file_path in directory.iterdir():
        if file_path.is_file() and file_path.suffix.lower() in image_extensions:
            count += 1

    return count


def create_padim_model(
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    backbone: str = "resnet18",
    layers: List[str] | None = None,
) -> Padim:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
    if layers is None:
        layers = ["layer1", "layer2", "layer3"]
    pre_processor = Padim.configure_pre_processor(image_size=image_size)

    model = Padim(
        backbone=backbone,
        layers=layers,
        pre_trained=True,
        pre_processor=pre_processor,
    )

    return model


def train_test_padim_model(
    datamodule: datamodules,
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    batch_size: int = 32,
    num_workers: int = 2,
) -> None:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    logger = logging.getLogger(__name__)

    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("å…ƒç”»åƒã‚µã‚¤ã‚º: 640x480 (ã‚«ãƒ¡ãƒ©è§£åƒåº¦)")
    logger.info(f"ãƒªã‚µã‚¤ã‚ºå¾Œã‚µã‚¤ã‚º: {image_size} (å‡¦ç†åŠ¹ç‡ã®ãŸã‚)")
    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    logger.info(f"ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}")

    # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    datamodule.setup()
    logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
    logger.info("=" * 40)
    logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…è¨³")
    logger.info("=" * 40)

    try:
        if hasattr(datamodule, "train_dataset") and datamodule.train_dataset:
            train_size = len(datamodule.train_dataset)
            logger.info(f"å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {train_size} æš")

        if hasattr(datamodule, "val_dataset") and datamodule.val_dataset:
            val_size = len(datamodule.val_dataset)
            logger.info(f"æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {val_size} æš")

        if hasattr(datamodule, "test_dataset") and datamodule.test_dataset:
            test_size = len(datamodule.test_dataset)
            logger.info(f"ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {test_size} æš")

        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æƒ…å ±
        if hasattr(datamodule, "train_dataloader"):
            train_loader = datamodule.train_dataloader()
            if train_loader:
                logger.info(f"å­¦ç¿’ç”¨ãƒãƒƒãƒæ•°: {len(train_loader)} ãƒãƒƒãƒ")

        if hasattr(datamodule, "val_dataloader"):
            val_loader = datamodule.val_dataloader()
            if val_loader:
                logger.info(f"æ¤œè¨¼ç”¨ãƒãƒƒãƒæ•°: {len(val_loader)} ãƒãƒƒãƒ")

        if hasattr(datamodule, "test_dataloader"):
            test_loader = datamodule.test_dataloader()
            if test_loader:
                logger.info(f"ãƒ†ã‚¹ãƒˆç”¨ãƒãƒƒãƒæ•°: {len(test_loader)} ãƒãƒƒãƒ")

    except Exception as e:
        logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
        raise

    logger.info("=" * 40)

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆç”»åƒã‚µã‚¤ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
    logger.info(f"PaDiMãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­ï¼ˆç”»åƒã‚µã‚¤ã‚º: {image_size}ï¼‰")
    model = create_padim_model(image_size=image_size, backbone="resnet18")
    engine = Engine()

    # å­¦ç¿’å®Ÿè¡Œ
    logger.info("=" * 50)
    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
    logger.info("=" * 50)
    logger.info(f"å­¦ç¿’é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³: resnet18")
    logger.info("ç‰¹å¾´æŠ½å‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼: ['layer1', 'layer2', 'layer3']")
    logger.info("=" * 50)

    try:
        engine.fit(model=model, datamodule=datamodule)
        logger.info("=" * 50)
        logger.info("å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"å­¦ç¿’å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 50)
    except Exception as e:
        logger.error("=" * 50)
        logger.error("å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        logger.error("=" * 50)
        raise

    logger.info("=" * 30)
    logger.info("ãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 30)

    try:
        # testã‚’å®Ÿè¡Œ
        logger.info("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        test_results = engine.test(model=model, datamodule=datamodule)

        logger.info("=" * 30)
        logger.info("ãƒ†ã‚¹ãƒˆå®Œäº†")
        logger.info(f"ãƒ†ã‚¹ãƒˆçµæœ: {test_results}")
        logger.info("=" * 30)

    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        logger.warning("ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å­¦ç¿’ã¯æ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã¾ã™")

    logger.info("ğŸ‰ PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ğŸ‰")


def main():
    dataset_root = "./dataset"
    image_size = (224, 224)
    batch_size = 128
    num_workers = 2

    # ãƒ­ã‚°è¨­å®š
    logger = setup_logging()

    # Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼ˆdataset/good, dataset/defectï¼‰
    datamodule = Folder(
        name="padim_train",
        root=".",
        normal_dir="dataset/good",
        abnormal_dir="dataset/defect",
        train_batch_size=batch_size,
        num_workers=num_workers,
        val_split_mode="from_test",
        test_split_mode="from_dir",
    )
    logger.info(
        f"Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ (batch_size={batch_size}, num_workers={num_workers})"
    )

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ
    train_test_padim_model(
        datamodule=datamodule,
        image_size=tuple(image_size),
        batch_size=batch_size,
        num_workers=num_workers,
    )

    logger.info("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    return


if __name__ == "__main__":
    main()
